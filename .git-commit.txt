Keep halting regularizer active during gate warmup

- Scale the halting penalty by the raw gate mean so warmup no longer zeroes the objective when the trainer clamps `gate_scale`, preserving steady supervision for the halting head.
- Recomputed the halting-loss weighting path, verified syntax with `python -m compileall hybrid/src/hrm_lm/models/hybrid.py`, and documented the workflow extensively in `hybrid/llm.state`.
- Resolved PR branch conflicts, refreshed the statefile audit trail, and staged the halting fix for push back to `codex/review-hybrid-branch-for-training-anomalies`.
