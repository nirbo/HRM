Stabilize HRM halting dynamics and Mamba encoder path

- Hardened the Mamba2 encoder by wrapping each state-space layer with residual connections, LayerNorm, and padding-aware masking so CLS statistics no longer explode when batches include padding.
- Updated the language encoder to average only unpadded tokens for its summary vector, keeping gradients smooth when the HRM core consumes CLS states regardless of backend.
- Let the HRM recurrent core retain gradients across all cycles whenever halting is enabled, while gating the halting regularizer by the bridge openness so the loss stays inactive during warmup.
- Defaulted halting targets to an attainable value of 1.0 in shared configs and captured the full implementation log in llm.state along with compileall validation notes.
