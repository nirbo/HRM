Original Task Description:
- Investigate why RWKV7 HRM fine-tuned model generation outputs <unk> tokens; ensure tokenizer loads correctly and generation works.
- Additional directive: diagnose whether QLoRA adapters or HRM integration introduce NaN logits despite base model health.

Plan:
1. Inspect QLoRA adapter tensors for NaN/Inf values to confirm adapter parameter health. [in_progress]
2. Simulate zeroed adapters (base + HRM only) on sample inputs to isolate HRM-induced instabilities. [pending]
3. Run LoRA adapters with HRM bypassed (gate forced closed) to see if NaNs persist without HRM contribution. [pending]
4. Compare logits statistics across scenarios and log findings in statefile. [pending]

Action Log:
- Extended generator tooling to support RWKV tokenizer artifacts and NaN sanitisation; confirmed inference now completes (though quality remains low).
- Ephemerally loaded RWKV7 base (LoRA disabled) to benchmark prompt responses; one prompt produced coherent answer, confirming base checkpoint integrity.
- Updated plan to drill into adapter vs HRM instability sources.

Pending Steps:
- Step 1 underway.
- Steps 2-4 pending execution.

Current State Summary:
- Need to determine whether adapter weights or HRM gating introduce NaNs when combined with RWKV kernels despite base stability.

Issues / Risks:
- Modifying adapter tensors must remain ephemeral to avoid contaminating saved checkpoints; ensure any zeroing occurs on copies and is reverted.
Action Log:
- Step 1: Loaded LoRA+HRM checkpoint (runs/rwkv7-hrm-gsm8k/best-model/model.pt) on CPU and scanned all parameters; no NaN or Inf values found in adapters or base weights.
Action Log:
- Step 2: Zeroed all LoRA adapter weights in-memory while keeping HRM active; forward pass on GSM8K sample still produced NaNs in logits, implicating non-adapter components (HRM or gating path).
Action Log:
- Step 3: Forced HRM gate scale/bias to zero while keeping LoRA adapters active; logits remained NaN, indicating instability originates upstream of HRM gating.
Action Log:
- Step 4: Compared logits across scenarios. Observations:
  * LoRA+HRM (original weights) ⇒ NaN logits on GSM8K sample.
  * LoRA zeroed, HRM active ⇒ NaN persists.
  * LoRA active, HRM gate forced closed ⇒ NaN persists.
  * Base RWKV-7 (LoRA disabled, no HRM) ⇒ logits finite.
  Conclusion: instability is triggered inside RWKV stack when HRM-integrated encoder is active, independent of LoRA weights or gate output.

Current State Summary:
- Adapter parameters are numerically clean. NaNs manifest whenever HRM-enabled RWKV encoder runs (even without LoRA influence or with gate closed), while the standalone base model remains stable. Root cause likely lies in HRM-to-RWKV interaction (masking, kernel inputs) rather than parameter corruption.

Issues / Risks:
- Need deeper inspection of HRM integration (mask shapes, chunk alignment, kernel backend) to eliminate NaNs without relying on logit sanitisation.
Action Log:
- Instrumented `rwkvt.rwkv7.att.RUN_CUDA_RWKV7g`; kernel inputs/outputs remain finite (no NaNs), confirming RWKV kernel is numerically stable under HRM drive.
- Captured encoder mask stats: shape (1,160), values in [0,1], no NaNs—mask alignment is correct.
- Drilled into transformer decoder: NaNs arise in the self-attention of decoder layer 0 when using boolean causal mask. Converting the causal mask to float (with -inf on the upper triangle) eliminates NaNs across the entire forward pass.
- Verified full model emits finite logits when decoder mask is float; NaNs disappear even without modifying HRM/LoRA.

Current State Summary:
- Root cause of NaNs traced to `TransformerDecoder.forward` constructing a boolean `tgt_mask`. PyTorch's multihead attention treats mixed mask dtypes poorly, yielding NaNs. Using a float mask resolves the instability.

Issues / Risks:
- Need permanent code fix: update decoder to supply float causal mask (and optionally align key padding mask dtypes) before re-running training or inference.
Action Log:
- Patched TransformerDecoder to build float-valued causal mask (upper triangle -inf) to preserve stability with PyTorch MHA.
- Re-ran inference on fine-tuned hybrid checkpoint; logits now finite (min -2.3353, max 7.1254) and no NaNs observed.

Pending Steps:
- Update plan steps tracked separately (retest completed).
Action Log:
- Ran three post-fix generations via CLI. Outputs remain gibberish despite logits being finite, consistent with model not being trained for dialog quality.
