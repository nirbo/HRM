Title: Stabilize generation masks and tokenizer loading

Summary:
- expand inference tokenizer loader to read RWKV vocab artifacts and sanitize logits/probabilities before sampling.
- switch encoder/decoder attention masks to boolean and convert transformer causal mask to float -inf values to avoid NaNs.
- update rwkv7_lora defaults (lr scheduler, gate bias/scale, halting weight) and refresh llm.state tracking entries.

Testing:
- PYTHONPATH=src ./venv/bin/python -m compileall src/hrm_lm/inference/generate.py
- PYTHONPATH=src TORCH_CUDA_ARCH_LIST=12.0+PTX ./venv/bin/python -m hrm_lm.inference.generate --config runs/rwkv7-hrm-gsm8k/best-model/config.yaml --checkpoint runs/rwkv7-hrm-gsm8k/best-model/model.pt --prompt "Sanity" --chat_template canonical --max_new_tokens 32 --device cuda
