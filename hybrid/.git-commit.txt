Convert FineWeb derivative into HRM-LM token dataset

Summary
- Added `scripts/prepare_language_dataset.py` to stream parquet rows into token triplets with the simple tokenizer, producing `train/val` JSONL files plus vocab metadata.
- Documented dataset preparation workflow in `TRAINING.md` and added required deps (`pyarrow`, `pandas`) to `requirements.txt`.
- Introduced basic `.gitignore` entries for datasets and run artifacts.

Outputs
- Generated `datasets/anothy1-fineweb-edu-cleaned-simplified/processed/{train,val}.jsonl`, `tokenizer.json`, and `meta.json` (kept untracked).
