Title: Fix RWKV7 kernel overrides for bf16 compatibility

Summary:
- ensure wind and flash-linear RWKV kernel wrappers cast r/w/k/v/a/b tensors to bf16 before invocation.
- patch rwkvt.rwkv7.att RUN_CUDA_RWKV7g reference so backend overrides propagate beyond module import.
- note dtype fix in statefile after verifying training run (now progresses to GPU OOM rather than dtype assert).

Testing:
- PYTHONPATH=src ./venv/bin/python -m compileall src/hrm_lm/models/rwkv7_backend.py
- PYTHONPATH=src timeout 30s ./venv/bin/python -m hrm_lm.training.train --config src/hrm_lm/configs/rwkv7_lora.yaml --dataset datasets/openai-gsm8k-socratic/ --batch_size 16 --steps 441 --learning_rate 0.0002 --warmup_steps 62 --lr_min_ratio 0.02 --grad_clip 1.5 --val_every 100 --run_name rwkv7-hrm-gsm8k --checkpoint_limit 3 --mixed_precision bf16 --eval_batch_size 16 --log_steps 1 --dataset_workers 30 --save_best_model --max_seq_len 1024 --max_val_samples 53 --eval_loss_patience 6 --patience_grace_steps 200 --hrm_gate_warmup_steps 100 --grad_accum_steps 1 (expected to stop via timeout after hitting CUDA OOM, confirming dtype assert resolved)
