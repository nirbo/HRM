Title: Freeze HRM/decoder modules by default for LoRA-only adapters

Summary:
- extend HRMLanguageModel to accept decoder_cfg, bridge trainable flags, and freeze HRM/decoder/bridge modules when trainable=false (new defaults).
- pass decoder_cfg from trainer and update rwkv7_lora.yaml (trainable flags false, LoRA rank kept 8).
- record evaluation in statefile; trainable params drop to ~7M (LoRA only).

Testing:
- PYTHONPATH=src timeout 60s ./venv/bin/python -m hrm_lm.training.train --config src/hrm_lm/configs/rwkv7_lora.yaml --dataset datasets/openai-gsm8k-socratic/ --batch_size 2 --grad_accum_steps 16 --steps 5 --learning_rate 0.0002 --warmup_steps 31 --lr_min_ratio 0.02 --grad_clip 1.5 --val_every 200 --run_name mem-check2 --checkpoint_limit 1 --mixed_precision bf16 --eval_batch_size 2 --log_steps 1 --dataset_workers 4 --save_best_model --max_seq_len 512 --max_val_samples 16 --eval_loss_patience 6 --patience_grace_steps 50 --hrm_gate_warmup_steps 100 --grad_checkpoint --optimizer tiger --override model.encoder.kernel_preference=fla_chunk
