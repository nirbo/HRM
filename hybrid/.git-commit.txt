Align decoder targets and modernize fp16 scaler

Summary
- Adjust synthetic dataset builder so decoder inputs (drop EOS) and labels (drop BOS) share sequence length, fixing cross-entropy shape mismatch during training.
- Switch trainer to `torch.amp.GradScaler` with explicit fp16 enable flag, eliminating deprecation warning.
- Verified via `uv run` synthetic training (200 steps) plus full pytest run.

Files
- src/hrm_lm/data/synthetic.py — align decoder/label sequences.
- src/hrm_lm/training/train.py — updated GradScaler logic.
- .git-commit.txt — summary for this commit.
