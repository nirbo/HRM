Extend trainer scheduling, logging, and CLI flexibility

Summary
- Rewrote trainer loop with Rich-based logging, linear warmup, resume-aware scheduling, and total-steps/epochs support.
- Added CLI overrides for learning rate, warmup steps, epochs, max sequence length, log frequency, and optimizer choice (AdamW / AdamW8bit).
- Improved dataset truncation, ETA/speed reporting, gradient norm metrics, and evaluation messaging.
- Documented new flags and workflow, added Rich dependency, and validated via dry run, sample training run, and pytest.

Files
- src/hrm_lm/training/train.py — major update (optimizer selection, warmup, epochs, rich logging, seq truncation, etc.).
- TRAINING.md — documented new CLI options and logging behavior.
- requirements.txt — added `rich` dependency.
- llm.state — updated task log (not committed).
