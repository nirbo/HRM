Abort on non-finite loss with batch dump

- When the forward pass returns NaN/Inf, save the offending batch (inputs, masks, metrics, lr) to `nan_batch_step_XXXXXX.pt` inside the run directory.
- Raise immediately after dumping so training halts; this prevents corrupted weights and gives us an artifact for offline debugging.
- Logged the behavior change in `hybrid/llm.state` and refreshed the backup so future resumes mention the abort behaviour.
