Improve allocator setup and prompt-primed inference

- set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True inside train.py to mitigate long-run CUDA fragmentation
- prime decoder with prompt tokens in inference/generate.py so completions extend the input instead of restarting from BOS
- log the OOM guidance and inference update in llm.state for reproducibility
