Title: Extend prepare_dataset to read JSONL inputs

Summary:
- Teach scripts/prepare_language_dataset.py to consume .json/.jsonl/.jsonl.gz sources in addition to parquet.
- Add prompt/response concatenation helpers and reuse tokenizer training across mixed formats.
- Update llm.state with the change.

Files Touched:
- scripts/prepare_language_dataset.py
- llm.state
- .git-commit.txt
- llm.state.bak (local backup, untracked)

Validation:
- python -m compileall scripts/prepare_language_dataset.py
- PYTHONPATH=src ./venv/bin/python scripts/prepare_language_dataset.py --source datasets/combined --dest datasets/combined/processed --tokenizer datasets/combined/processed/tokenizer.json --vocab-size 131072 --max-seq-len 2048 --tokenizer-num-threads 30 --tokenizer-batch-size 4096 --val-ratio 0.1 --seed 1337 --max-files 1

Follow-ups:
- None.
