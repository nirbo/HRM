Title: Add gradient checkpointing support for RWKV7 training

Summary:
- expose --grad_checkpoint CLI flag and propagate to config; tighten encoder/decoder seq_len syncing.
- wire RWKV7 encoder to optionally wrap blocks with torch.utils.checkpoint (use_reentrant=False) for memory relief.
- document new toggle and dataset settings in rwkv7_lora.yaml (grad_checkpoint:false default, ctx=512).
- log activation memory note in llm.state.

Testing:
- PYTHONPATH=src ./venv/bin/python -m compileall src/hrm_lm/models/rwkv7_backend.py src/hrm_lm/training/train.py
