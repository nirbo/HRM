Add optimizer selection to trainer

Summary
- Introduced `--optimizer` flag with support for standard AdamW and bitsandbytes AdamW8bit via `make_optimizer` helper.
- Added documentation for optimizer choices, including installation note for 8-bit mode.
- Validated via dry-run, sample training run, and pytest.

Files
- src/hrm_lm/training/train.py — optimizer factory, CLI flag usage.
- TRAINING.md — documented optimizer options.
- llm.state — updated log (not committed).
