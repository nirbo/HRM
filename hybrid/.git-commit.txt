Implemented defensive CUDA graph integration for MoE training.

Summary of changes:
- Documented new troubleshooting session and resolution steps inside llm.state and refreshed llm.state.bak backup.
- Enabled CUDA graph usage flag in the default trainer config while allowing runtime auto-disable when unsupported.
- Tagged encoder backends (Mamba2 and MoE Transformer) as graph-incompatible and propagated capability metadata through HRMLanguageModel.
- Adjusted MoE feed-forward routing to drop host-side `mask.any()` checks in favor of capture-friendly tensor indexing.
- Hardened the training loop: early eager warmup, guarded graph initialization with graceful fallback to eager execution, and preserved logging when graphs are unavailable.

Testing & validation:
- `python -m compileall src/hrm_lm/models/encoder.py src/hrm_lm/models/hybrid.py src/hrm_lm/models/transformer_layers.py src/hrm_lm/training/train.py`
