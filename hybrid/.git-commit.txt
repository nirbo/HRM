Improve CUDA timeout recovery

Summary:
- wrap the retry path for CUDA timeouts in a guarded synchronize/empty_cache/ipc_collect with a short pause so we can retry even if the first sync call fails
- log the fallback behaviour and mention the enhanced timeout handling in TRAINING.md
- capture the statefile update noting the new timeout recovery logic

Files touched:
- src/hrm_lm/training/train.py
- TRAINING.md
- llm.state
