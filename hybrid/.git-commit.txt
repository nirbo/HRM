Integrate RWKV-PEFT adapters into RWKV7 backend.

- added PEFT config normalisation + adapter initialisation to rwkv7 backend
- auto-disable torch.compile, enforce bf16 weights, freeze base params, load LoRA/PiSSA/DiSHA checkpoints
- support quantised adapters (QLoRA) and trainer-side dtype handling
- made LM encoder bypass embeddings for rwkv7 and cast outputs for HRM bridge
- documented PEFT options in RWKV.md and added sample config configs/rwkv7_lora.yaml
- validated dry-run training path with LoRA enabled
