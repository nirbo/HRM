Handle eval OOM via autocast and document

Summary:
- reuse the training autocast context during validation forwards so bf16/fp16 runs avoid fp32 cross-entropy buffers that triggered CUDA OOMs
- document the behavior in TRAINING.md notes for future operators
- log all actions in llm.state (dataset filtering, autocast patch, documentation updates)

Files touched:
- src/hrm_lm/training/train.py
- TRAINING.md
- llm.state
