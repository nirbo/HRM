Title: Add Transformer MoE option

Summary:
- Replace the stock Transformer encoder with a custom block that supports mixture-of-experts feed-forward layers, including gating, top-k routing, capacity control, and load-balancing aux loss.
- Extend LMEncoder/Hybrid plumbing to surface the MoE auxiliary loss and allow weighting via config, while keeping the mamba path unchanged and updating HRM/Mamba fallbacks for the new encoder API.
- Add `model.encoder.moe` settings to the default config and refresh documentation/state tracking to explain how to enable and tune the MoE backend.

Testing:
- PYTHONPATH=src ./venv/bin/python -m hrm_lm.training.train --dry_run
- Transformer+MoE smoke test via inline Python script (build model, forward pass)
