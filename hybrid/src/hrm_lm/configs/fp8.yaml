# 2-space indent
model:
  vocab_size: 131072
  d_model: 512
  n_heads: 8
  dropout: 0.0
  encoder:
    backend: transformer  # transformer | mamba2
    n_layers: 8
    max_seq_len: 2048
    moe:
      enabled: false
      num_experts: 8
      top_k: 4
      capacity_factor: 1.25
      ff_multiplier: 4.0
      dropout: 0.0
      aux_loss_weight: 0.01
  decoder:
    n_layers: 8
    max_seq_len: 1024
  hrm:
    d_model: 512
    h_len: 8
    l_len: 64
    h_layers: 2
    l_layers: 2
    h_cycles: 4
    l_steps: 8
  approx_grad: one_step  # one_step | bptt
  use_halting: true
  halting_weight: 0.05
  halting_target: 1.0
  out_dim: 512
  deep_supervision: false
  ds_weight: 0.2
  gate_scale: 1.0
  gate_bias: 0.0
bridge:
  type: prefix  # prefix | cross_attn
  prefix_len: 8
optim:
  lr: 2.5e-4
  weight_decay: 0.0
  betas: [0.9, 0.95]
train:
  seed: 42
  batch_size: 2
  eval_batch_size: 2
  seq_len: 256
  tgt_len: 64
  mixed_precision: fp8
  enable_tf32: true
  use_cuda_graphs: false
  fp8:
    enabled: true
    warmup_steps: 0
    format: e4m3  # e4m3 | e5m2
    recipe:
      margin: 0
      interval: 1
      amax_history_len: 16
      amax_compute_algo: max

profiling:
  nvtx:
    enabled: true
    include_data: true
    include_eval: true
