model:
  vocab_size: 131072
  d_model: 512
  n_heads: 8
  dropout: 0.0
  encoder:
    backend: transformer
    n_layers: 8
    max_seq_len: 2048
  decoder:
    n_layers: 8
    max_seq_len: 1024
  hrm:
    d_model: 512
    h_len: 8
    l_len: 64
    h_layers: 2
    l_layers: 2
    h_cycles: 4
    l_steps: 8
    approx_grad: one_step
    out_dim: 512
    use_halting: true
    halting_weight: 0.002
    deep_supervision: false
    ds_weight: 0.0
bridge:
  type: prefix
  prefix_len: 8
optim:
  lr: 0.00025
  weight_decay: 0.0
  betas:
  - 0.9
  - 0.95
train:
  seed: 2906
  batch_size: 22
  eval_batch_size: 22
  seq_len: 512
  tgt_len: 512
  mixed_precision: bf16
