# 2-space indent
model:
  vocab_size: 131072
  d_model: 512
  n_heads: 8
  dropout: 0.0
  encoder:
    backend: transformer # transformer | mamba2
    n_layers: 8
    max_seq_len: 2048
    moe:
      enabled: true
      num_experts: 8
      top_k: 4
      capacity_factor: 1.25
      ff_multiplier: 4.0
      dropout: 0.0
      aux_loss_weight: 0.01
  decoder:
    n_layers: 8
    max_seq_len: 1024
  hrm:
    d_model: 512
    h_len: 8
    l_len: 64
    h_layers: 2
    l_layers: 2
    h_cycles: 8
    l_steps: 8
    approx_grad: one_step # one_step | bptt
    use_halting: true
    halting_weight: 0.04
    halting_target: 3.0
    gate_scale: 1.0
    gate_bias: 0.02
    out_dim: 512
    deep_supervision: false
    ds_weight: 0.2
bridge:
  type: prefix # prefix | cross_attn
  prefix_len: 8
optim:
  name: came
  lr: 0.0002
  weight_decay: 0.0
  betas: [0.9, 0.95]
  kwargs: {}
loss:
  name: cross_entropy
  kwargs: {}

train:
  lr_scheduler:
    name: warmup_stable_decay
    kwargs:
      num_warmup_steps: 2000
      num_stable_steps: 152800   # adjust downward if you trim total steps
      num_decay_steps: 50925
      cooldown_type: '1-sqrt'    # default; cosine also works if you prefer smoother tail
      min_lr_ratio: 0.02         # floor for the tail (tweak as needed)
  seed: 12
  batch_size: 8
  eval_batch_size: 8
  grad_accum_steps: 1
  seq_len: 256
  tgt_len: 64
  mixed_precision: bf16
  enable_tf32: true
  use_cuda_graphs: false
  fp8:
    enabled: false
    warmup_steps: 512
    format: e4m3 # e4m3 | e5m2
    recipe:
      margin: 0
      interval: 1
      amax_history_len: 16
      amax_compute_algo: max
  extra_eval_slices: []
#    - name: mbpp
#      path: datasets/reasoning/mbpp/processed

profiling:
  nvtx:
    enabled: false
    include_data: true
    include_eval: true
