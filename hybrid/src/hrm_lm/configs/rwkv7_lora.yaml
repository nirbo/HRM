# Sample configuration for RWKV7 + HRM with LoRA/QLoRA adapters
model:
  vocab_size: 65536
  d_model: 2048
  encoder:
    backend: rwkv7
    n_layers: 24
    max_seq_len: 512
    checkpoint_path: models/blinkdl-rwkv7-g1a-1.5b/rwkv7-g1a-1.5b-20250922-ctx4096.pth
    dim_att: 2048
    dim_ffn: 7168
    head_size_a: 64
    head_size_divisor: 8
    kernel_preference: wind_longhead
    grad_checkpoint: false
    detach_output: true
    peft:
      type: none         # options: none | lora | qlora | pissa | disha
      quantization: none # set to nf4/int8/fp8/etc for QLoRA-style training
      freeze_non_peft: true
      train_embeddings: false
      train_head: false
      train_layer_norms: false
      train_parts: []
      lora:
        r: 8
        alpha: 32
        dropout: 0.00
        load_path: ""    # optional path to resume LoRA weights
      pissa:
        r: 16
        svd_niter: 4
        load_path: ""
        init_path: ""
      disha:
        mode: bone       # bone | bat
        r: 64
        load_path: ""
  decoder:
    n_layers: 8
    max_seq_len: 512
    trainable: false
  hrm:
    d_model: 2048
    h_len: 8
    l_len: 64
    h_layers: 6
    l_layers: 6
    h_cycles: 6
    l_steps: 4
    approx_grad: one_step
    use_halting: true
    halting_weight: 0.05
    halting_target: 1.0
    out_dim: 2048
    deep_supervision: false
    ds_weight: 0.2
    gate_scale: 1.0
    gate_bias: 0.0
    trainable: true
bridge:
  type: prefix
  prefix_len: 16
  trainable: true
optim:
  name: adamw
  lr: 1.5e-4
  weight_decay: 0.0
  betas: [0.9, 0.95]
  kwargs: {}
loss:
  name: cross_entropy
  kwargs: {}
train:
  lr_scheduler:
    name: warmup_stable_decay
    kwargs:
      num_warmup_steps: 31       # ~1000 raw micro-steps / effective batch 32
      num_stable_steps: 137      # adjusted for batch_size=2 & grad_accum=16 (total optimizer steps â‰ˆ247)
      num_decay_steps: 79        # completes 247 optimizer steps (31 + 137 + 79)
      cooldown_type: '1-sqrt'    # default; cosine also works if you prefer smoother tail
      min_lr_ratio: 0.02         # floor for the tail (tweak as needed)
  seed: 42
  batch_size: 2
  eval_batch_size: 2
  grad_accum_steps: 16
  seq_len: 256
  tgt_len: 64
  mixed_precision: bf16
  enable_tf32: true
  use_cuda_graphs: false
  fp8:
    enabled: false
    warmup_steps: 512
    format: e4m3
    recipe:
      margin: 0
      interval: 1
      amax_history_len: 16
      amax_compute_algo: max
profiling:
  nvtx:
    enabled: false
    include_data: true
    include_eval: true
